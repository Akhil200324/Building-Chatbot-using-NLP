{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WE', 'ARE', 'IN', 'COMPUTERLAB', 'IN', 'THE', 'UNIVERSITY']\n",
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "My name is chitti.I will answer your queries about Chatbots.        If you want to exit, say Bye!\n",
      "Start talking\n",
      "You said:stop\n",
      "stop\n",
      "I am sorry! I don't understand you\n",
      "Start talking\n",
      "I cannot hear you\n",
      "\n",
      "I am sorry! I don't understand you\n",
      "Start talking\n",
      "I cannot hear you\n",
      "\n",
      "I am sorry! I don't understand you\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#Stemming vs Lemmatization\n",
    "a = 'wolves'\n",
    "#nltk.download('all') -->should be done to have all corpus\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#lemmatization\n",
    "wnl = WordNetLemmatizer()\n",
    "wnl.lemmatize(a)\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "ps.stem(a)\n",
    "#Understanding Stopwords and Tf-idf Vectorization\n",
    "from nltk.corpus import stopwords\n",
    "stopwords\n",
    "#nltk.download('stopwords') this should be done if above stopwords \n",
    "#are not loaded\n",
    "stopwords.words(\"english\")\n",
    "#Give your desired sentence or group of words\n",
    "a = [\"We\",\"are\",\"in\",\"ComputerLab\",\"in\",\"the\",\"University\"]\n",
    "#List Comprehension\n",
    "#a= [exprsn for var in function]\n",
    "c=[]\n",
    "for i in a:\n",
    "   #print(i.upper())\n",
    "    c.append(i.upper())\n",
    "print(c)\n",
    "d =[i.upper() for i in a]\n",
    "d\n",
    "e=[]\n",
    "for i in range(1,11):\n",
    "    #print(i**2)\n",
    "    e.append(i**2)\n",
    "print(e)\n",
    "d = [i**2 for i in range(1,11)]\n",
    "d\n",
    "a\n",
    "english_stop = set(stopwords.words('english'))\n",
    "#print(english_stop)\n",
    "final_data = [i for i in a if i not in english_stop]\n",
    "final_data\n",
    "#Tf-idf Vectorization and Cosine Similarity\n",
    "\n",
    "### TF-IDF Approach\n",
    "#A problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content”. Also, it will give more weight to longer documents than shorter documents.\n",
    "\n",
    "#One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or TF-IDF for short, where:\n",
    "\n",
    "\"\"\"**Term Frequency: is a scoring of the frequency of the word in the current document.**\"\"\"\n",
    "\n",
    "'''\n",
    "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "\n",
    "\n",
    "**Inverse Document Frequency: is a scoring of how rare the word is across documents.**\n",
    "\n",
    "```\n",
    "IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "```\n",
    "### Cosine Similarity\n",
    "\n",
    "Tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus\n",
    "\n",
    "```\n",
    "Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n",
    "```\n",
    "where d1,d2 are two non zero vectors.\n",
    "\n",
    "#Steps involved in Building Chatbot\n",
    "-->Installing packages (nltk)\n",
    "-->Getting the raw data (create our own corpus)\n",
    "-->Preprocessing the text (Tokenization,Removal of stopwords,\n",
    "Stemming & Lemmatization)\n",
    "-->Keyword Matching\n",
    "-->Generate Responses\n",
    "#import the packages\n",
    "'''\n",
    "import nltk\n",
    "import random\n",
    "from gtts import gTTS\n",
    "import speech_recognition as sr\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import playsound\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install numpy\n",
    "#pip install scikit-learn\n",
    "import numpy\n",
    "from time import ctime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os.path\n",
    "#pip install -U scikit-learn\n",
    "#pip install -U numpy\n",
    "#Read the raw data from the text file\n",
    "f = open('chatbot.txt','r',errors=\"ignore\") #default we have 'r' mode\n",
    "#f.read()\n",
    "#Convert the entire raw text into lower case\n",
    "f = f.read() #this gives the entire data from text file in string format\n",
    "#type(f)\n",
    "f=f.lower() #to avoid case-sensitive errors\n",
    "'''\n",
    "The main issue with text data is that it is all in text format (strings). However, the Machine learning algorithms need some sort of numerical feature vector in order to perform the task. So before we start with any NLP project we need to pre-process it to make it ideal for working. Basic text pre-processing includes:\n",
    "\n",
    "* Converting the entire text into **uppercase** or **lowercase**, so that the algorithm does not treat the same words in different cases as different\n",
    "\n",
    "* **Tokenization**: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n",
    "#Processing the rawtext -->Tokenization\n",
    "#WordTokenization and SentenceTokenization'''\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "word_tokens = word_tokenize(f)\n",
    "#word_tokens\n",
    "sent_tokens = sent_tokenize(f)\n",
    "sent_tokens[0]\n",
    "## Preprocessing\n",
    "\n",
    "#We shall now define a function called LemTokens which will take as input the tokens and return normalized tokens.\n",
    "#Now we have tokens we will create a function to apply Lemmatization'\n",
    "lemmer = WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "import string\n",
    "#to remove punctuation marks and other special character cases\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "#The below function will accept lemmatized words and removes punctuations\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "## Keyword matching\n",
    "\n",
    "#Next, we shall define a function for a greeting by the bot i.e if a user’s input is a greeting, the bot shall return a greeting response.ELIZA uses a simple keyword matching for greetings. We will utilize the same concept here.\n",
    "#Keyword Matching by giving some basic input responses and getting greetings\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \n",
    "                      \"I am glad! You are talking to me\"]\n",
    "#Function to generate random responses from above greetings\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "#greeting(\"hey hai how are you?\")\n",
    "#To generate a response from our bot for input questions, the concept of document similarity will be used. We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry! I don’t understand you”\n",
    "#Generating Responses \n",
    "def response(user_response): #here the argument will be user data\n",
    "    robo_response='' #this will be chatbot response\n",
    "    sent_tokens.append(user_response) #sent_tokens ->Sentence Tokenizer\n",
    "    \n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, \n",
    "                               stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    \n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "\n",
    "    if(req_tfidf==0): #this will be activated if the given response is\n",
    "        #not present in the corpus \n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response  \n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "#Copy both listen() and respond() function from VirtualAssistant\n",
    "def listen():\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Start talking\") #own statement\n",
    "        audio = r.listen(source,phrase_time_limit=5)\n",
    "    data = \"\"\n",
    "    #Exception Handling\n",
    "    try:\n",
    "        data = r.recognize_google(audio,language='en-US')\n",
    "        print(\"You said:\"+data)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"I cannot hear you\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Request Failed\")\n",
    "    return data \n",
    "#To respond back with audio \n",
    "def respond(String):\n",
    "    print(String)\n",
    "    tts = gTTS(text = String,lang = 'en-US')\n",
    "    if os.path.exists(\"speech.mp3\"):\n",
    "        os.remove('speech.mp3')\n",
    "        tts.save('speech.mp3')\n",
    "    else:\n",
    "        tts.save(\"speech.mp3\")\n",
    "    playsound.playsound(\"speech.mp3\")\n",
    "#Final chat with our chatbot\n",
    "flag=True #initialized varaible to make loop going ahead\n",
    "respond(\"My name is chitti.I will answer your queries about Chatbots.\\\n",
    "        If you want to exit, say Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = listen()\n",
    "    user_response=user_response.lower()   \n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            respond(\"You are welcome..\")        \n",
    "        elif 'how are you' in user_response:\n",
    "            respond(\"I am Fine,thanks for asking\")\n",
    "        elif 'movie' in user_response:\n",
    "            respond(\"I do not watch many movies.Thanks for the concern\")\n",
    "        elif 'time' in user_response:\n",
    "            respond(ctime())\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                respond(greeting(user_response))\n",
    "            else:\n",
    "                print(user_response)\n",
    "                respond(response(user_response)) #this is our Rawformattedpart\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        respond(\"Bye! take care..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29440d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d0fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
